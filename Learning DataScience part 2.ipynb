{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out what's in your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number String Boolean\n",
      "0      1  First    True\n",
      "\n",
      "Series([], dtype: bool)\n",
      "  Number  String Boolean\n",
      "0      1   First    True\n",
      "1      2  Second   False\n",
      "\n",
      "Series([], dtype: bool)\n",
      "  Number  String Boolean\n",
      "0      1   First    True\n",
      "1      2  Second   False\n",
      "2      3   Third    True\n",
      "\n",
      "Series([], dtype: bool)\n",
      "  Number  String Boolean\n",
      "0      1   First    True\n",
      "1      2  Second   False\n",
      "2      3   Third    True\n",
      "3      3   Third    True\n",
      "\n",
      "3    True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "xml = objectify.parse(open('XMLData2.xml'))\n",
    "root = xml.getroot()\n",
    "df = pd.DataFrame(columns=('Number', 'String', 'Boolean'))\n",
    "\n",
    "for i in range(0,4):\n",
    "    obj = root.getchildren()[i].getchildren()\n",
    "    row = dict(zip(['Number', 'String', 'Boolean'],\n",
    "                   [obj[0].text, obj[1].text,\n",
    "                   obj[2].text]))\n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = i\n",
    "    df = df.append(row_s)\n",
    "    \n",
    "    \n",
    "    search = pd.DataFrame.duplicated(df)\n",
    "    print(df)\n",
    "    print()\n",
    "    print(search[search == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number  String Boolean\n",
      "0      1   First    True\n",
      "1      2  Second   False\n",
      "2      3   Third    True\n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "\n",
    "xml = objectify.parse(open('XMLData2.xml'))\n",
    "root = xml.getroot()\n",
    "df = pd.DataFrame(columns=('Number', 'String', 'Boolean'))\n",
    "for i in range(0,4):\n",
    "    obj = root.getchildren()[i].getchildren()\n",
    "    row = dict(zip(['Number', 'String', 'Boolean'],\n",
    "                  [obj[0].text, obj[1].text, obj[2].text]))\n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = i\n",
    "    df = df.append(row_s)\n",
    "    \n",
    "print(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Map and Data Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      B                                            \\\n",
      "  count mean       std  min   25%  50%   75%  max   \n",
      "A                                                   \n",
      "0   5.0  3.0  1.581139  1.0  2.00  3.0  4.00  5.0   \n",
      "1   2.0  3.5  2.121320  2.0  2.75  3.5  4.25  5.0   \n",
      "\n",
      "      C                                            \n",
      "  count mean       std  min   25%  50%   75%  max  \n",
      "A                                                  \n",
      "0   5.0  2.8  1.788854  1.0  1.00  3.0  4.00  5.0  \n",
      "1   2.0  2.5  0.707107  2.0  2.25  2.5  2.75  3.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 55)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [0,0,0,0,0,1,1],\n",
    "                   'B': [1,2,3,5,4,2,5],\n",
    "                   'C': [5,3,4,1,1,2,3]})\n",
    "\n",
    "a_group_desc = df.groupby('A').describe()\n",
    "print(a_group_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stacked version on this data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                B         C\n",
      "A                          \n",
      "0 count  5.000000  5.000000\n",
      "  mean   3.000000  2.800000\n",
      "  std    1.581139  1.788854\n",
      "  min    1.000000  1.000000\n",
      "  25%    2.000000  1.000000\n",
      "  50%    3.000000  3.000000\n",
      "  75%    4.000000  4.000000\n",
      "  max    5.000000  5.000000\n",
      "1 count  2.000000  2.000000\n",
      "  mean   3.500000  2.500000\n",
      "  std    2.121320  0.707107\n",
      "  min    2.000000  2.000000\n",
      "  25%    2.750000  2.250000\n",
      "  50%    3.500000  2.500000\n",
      "  75%    4.250000  2.750000\n",
      "  max    5.000000  3.000000\n"
     ]
    }
   ],
   "source": [
    "stacked = a_group_desc.stack()\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see only the number of items in each series and their mean. Use this code to reduce the size of the information output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      B          C     \n",
      "  count mean count mean\n",
      "A                      \n",
      "0   5.0  3.0   5.0  2.8\n",
      "1   2.0  3.5   2.0  2.5\n"
     ]
    }
   ],
   "source": [
    "print(a_group_desc.loc[:,(slice(None),['count','mean']),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables have specific number of values, which makes them incredibly valuable in performing a number of data science tasks. For example, imagine trying to find value that are out of range in a huge dataset. In this example, you see one method for creating a categorical variable and then using it to check whether some data falls within the specified limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Blue\n",
      "1      Red\n",
      "2    Green\n",
      "dtype: category\n",
      "Categories (3, object): [Red, Blue, Green]\n",
      "\n",
      "0      NaN\n",
      "1    Green\n",
      "2      NaN\n",
      "3     Blue\n",
      "4      NaN\n",
      "dtype: category\n",
      "Categories (3, object): [Red, Blue, Green]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "car_colors = pd.Series(['Blue', ' Red', 'Green'],\n",
    "                      dtype='category')\n",
    "\n",
    "car_data = pd.Series(\n",
    "    pd.Categorical(\n",
    "        ['Yellow', 'Green', 'Red', 'Blue', 'Purple'],\n",
    "                 categories=car_colors, ordered=False))\n",
    "\n",
    "\n",
    "find_entries = pd.isnull(car_data)\n",
    "\n",
    "print(car_colors)\n",
    "print()\n",
    "print(car_data)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example begins by creating a categorical variable, car_colors. The variable contains the value Blue, Red and Green as colors that are acceptable for a car. Notice that you must specify a dtype property value of category. The next step is to create another series. This one uses a list of actual car colors, named car_data, as imput. Not all the car colors match the predefined acceptablevalues. When this problem occurs, pandas output Not a Number(NaN) instead of the car color.\n",
    "\n",
    "\n",
    "Of course , you could search the list manually for the nonconforming cars, but the easiest method is to have pandas do the work for you. In this case, you ask pandas which entries are null using isnull() and place them in find_entries. You can see from the output of the code above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "0     Blue\n",
    "1      Red\n",
    "2    Green\n",
    "dtype: category\n",
    "Categories (3, object): [Red, Blue, Green]\n",
    "\n",
    "0      NaN\n",
    "1    Green\n",
    "2      NaN\n",
    "3     Blue\n",
    "4      NaN\n",
    "dtype: category\n",
    "Categories (3, object): [Red, Blue, Green]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the list of car_data outputs, you can see thats entries 0 and 4 equal to NaN. The output from find_entries verifies this fact for you. If this were a large dataset, you could quickly locate and correct errant entries in the data set before performing an analysis on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when the naming of categories you use is inconvinient or otherwise wrong for a particular need as needed using the technique shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Purple\n",
      "1    Yellow\n",
      "2     Mauve\n",
      "3    Purple\n",
      "4     Mauve\n",
      "dtype: category\n",
      "Categories (3, object): [Purple, Yellow, Mauve]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "car_colors = pd.Series(['Blue', 'Red', 'Green'], \n",
    "                       dtype='category')\n",
    "car_data = pd.Series(\n",
    "    pd.Categorical(\n",
    "        ['Blue', 'Green', 'Red', 'Blue', 'Red'],\n",
    "        categories=car_colors, ordered=False))\n",
    "\n",
    "\n",
    "car_colors.cat.categories = [\"Purple\", \"Yellow\", \"Mauve\"]\n",
    "car_data.cat.categories = car_colors\n",
    "\n",
    "print(car_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need to do is set cat.categories property to a new value, as shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular categorical level might be too small to offer significant data for analysis. Perhaps there are only a few of the values, which may not be enoughto create a statistical difference. In this case, combining several small categories might offer better  analysis result. The following exemple shows how to combine categories : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    Red\n",
      "3    Red\n",
      "5    Red\n",
      "dtype: category\n",
      "Categories (3, object): [Blue_Red, Red, Green]\n",
      "\n",
      "0    Blue_Red\n",
      "1    Blue_Red\n",
      "2       Green\n",
      "3    Blue_Red\n",
      "4       Green\n",
      "5    Blue_Red\n",
      "dtype: category\n",
      "Categories (2, object): [Green, Blue_Red]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "car_colors = pd.Series(['Blue', 'Red', 'Green'], \n",
    "                       dtype='category')\n",
    "car_data = pd.Series(\n",
    "    pd.Categorical(\n",
    "        ['Blue', 'Green', 'Red', 'Green', 'Red', 'Green'],\n",
    "        categories=car_colors, ordered=False))\n",
    "\n",
    "car_data.cat.categories = [\"Blue_Red\", \"Red\", \"Green\"]\n",
    "print (car_data[car_data.isin(['Red'])])\n",
    "\n",
    "car_data[car_data.isin(['Red'])] = 'Blue_Red'\n",
    "\n",
    "car_data.loc[car_data.isin(['Red'])] = 'Blue_Red'\n",
    "car_data.loc[car_data.isin(['Blue'])] = 'Blue_Red'\n",
    "car_data = car_data.cat.set_categories(\n",
    "    [\"Green\", \"Blue_Red\"])\n",
    "\n",
    "print()\n",
    "print(car_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this example shows you is that there is only one Blue item and only two Red item, but there are three Green items, which places Green in the majority. Combining Blue and Red together is a two-step process. First, you add the Blue_Red category to car_data. Then you change the Red and Blue entries to Blue_Red, which create the combined category. As a final step, you can remove the unneeded categories.\n",
    "\n",
    "However, before you can change the Red entries to Blue_Red entries, you must find them. This is where a combination of calls to isin(), which locate the Red entries, and loc[], which obtains their index, provides precisely what you need. The First print() statement show the result of this combination."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2    Red\n",
    "4    Red\n",
    "dtype: category\n",
    "Categories (4, object): [Blue, Red, Green, Blue_Red]\n",
    "\n",
    "0    Blue_Red\n",
    "1       Green\n",
    "2    Blue_Red\n",
    "3    Blue_Red\n",
    "4    Blue_Red\n",
    "dtype: category\n",
    "Categories (2, object): [Green, Blue_Red]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now three Blue_Red entries and three Green entries. The result is thath the levels are now combined as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Date In Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting date and time values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the correct date and time representation can make performing analysis a lot easier. For example, you ofter have to change the representation to obtain a correct sorting of values. Python provides two common method of formatting date and time. The first is to call str(), which simply turns a datetime value into a string without any formatting. The strftime(), you must provide a string containing special directives that define the formatting. You can find a listing of these directive at http://strftime.org/\n",
    "\n",
    "Now that you have some idea of how time and date conversion work, it's time to see an exemple create datetime object and them converts it into a string using two different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-16 13:27:25.974477\n",
      "Tue, 16, February, 2021\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now = dt.datetime.now()\n",
    "\n",
    "print(str(now))\n",
    "print(now.strftime('%a, %d, %B, %Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that using str() is the easiest approach. However, as shown by the output, it may not provide the output you need. Using strftime() is infinitely more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the right time transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time zone and differences in local time can cause all sorts of problems when performing analysis. For that matter, some types of calculation simply require a time shift in order to get the right results. No matter what the reason, you may need to transform one time into another time at some point. The following example  show some technique you can employ to perform the task :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:27:27\n",
      "15:27:27\n",
      "2:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now = dt.datetime.now()\n",
    "timevalue = now + dt.timedelta(hours=2)\n",
    "\n",
    "print(now.strftime('%H:%M:%S'))\n",
    "print(timevalue.strftime('%H:%M:%S'))\n",
    "print(timevalue-now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timedelta() function makes time transformation straightfoward. You can use any of these parameter names with timedelta() to change a time and date value.\n",
    "You can also manipulate time by performing additions or substraction on time values. You can even substract two time value to determinate the difference betwenn them Check the outpout of the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now is the local time , timevalue is two time zone different from this one, and there is a two-hour difference between the two times. You can perform all sorts of transformations using these technique to ensure that your analysis always showss precisely the time-oriented values you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data you receive is missing information in specific fields. For example, a customer record might be missing an age. If enough recorsds are missing entries, any analysis you perform will be skewed and the result of the analysis weighted in a unpredictable manner. Having a strategy for dealing with missing data is important. The following sections give some idea on how to work through these issue and produce better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the missing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding missing data in your dataset is essential to avoid getting incorrect results from analysis. The following code shows how you can obtain a listing of missing values without too much effort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4    False\n",
      "5     True\n",
      "6    False\n",
      "7    False\n",
      "dtype: bool\n",
      "\n",
      "2     NaN\n",
      "5    None\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = pd.Series([1, 2, np.NaN, 5, 6, None, 'Hello', 0])\n",
    "\n",
    "print(s.isnull())\n",
    "\n",
    "\n",
    "print()\n",
    "print(s[s.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset can represent missing data in several ways. In this example, you see missing data represented as np.NaN (Numpy Not a Number) and the python None value.\n",
    "\n",
    "\n",
    "Use the isnull() method to detect the missing values. The ouput of shows True when the value is missing. By adding an index into the dataset, you obtain just the entries that are missing. The example output above show the result of this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you figured out that your dataset is missing information, you need to consider what do to about it. Three possibilities are to ignore the issue, fill in the missing items, or remove (drop) the missing entries from the dataset. Ignoring the issue could lead to all sorts of problems for your analysis, so it's the option you will use least often. The following example shows one technique for filling in missing data or dropping the errant entries from the datasset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    3.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "6    3.0\n",
      "dtype: float64\n",
      "\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "s = pd.Series([1, 2, 3, np.NaN, 5, 6, None])\n",
    "print(s.fillna(int(s.mean())))\n",
    "print()\n",
    "print(s.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two method of interest are fillna(), wich fill in the missing entries, dropna(), which drop the missing entries. When using fillna(), you must provide a value to use for the missing data. This example use the mean of all the values, but you could choose a number of other approaches. dropna() will remove the missing value as shown in the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Stuff : Working with a series is straightfoward because the dataset is so simple. When working with DATAFRAME , however, the problem becomes more significantly more complicated. You still have the option of dropping the entire row. When a column is sparsely populated, you might drop the column insted. Filling in the data also becomes more complex because you must consider the dataset as a whole, in addition to the needs of the individual feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section hints at the processs of imputing missing data (ascribing characteristics based on how the data is used). The technique you use depends on the sort of data you're working with. For example, when working with a tree ensemble you may simply replace missing value with -1 and rely on the imputer ( a transformer algorithm used to complete misssing value) to define the best possible value for the missing data. The following example shows a ttechnique you can use to impute missing data values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "5    6.0\n",
      "6    7.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "s = [[1, 2, 3, np.NaN, 5, 6, None]]\n",
    "\n",
    "imp = SimpleImputer(missing_values= np.NAN, strategy= 'mean', fill_value=None, verbose=0, copy=True)\n",
    "\n",
    "imp.fit([[1, 2, 3, 4, 5, 6, 7]])\n",
    "\n",
    "x = pd.Series(imp.transform(s).tolist()[0])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, s is missing some values. The code create an SimpleImputer to replace these missing value. The missing_value parameter defines what to look for, which is np.NaN. you set the fill_value parameter to None to impute along the columns .\n",
    "\n",
    "mean : Replace the value by using the mean along the axis\n",
    "\n",
    "median : Replace the value by using the medium along the axis\n",
    "\n",
    "most_frequent: Replaces the value by using the most frequent value along the axis\n",
    "\n",
    "Before you can impute anything, you must provide statistic for the SimpleImputer to use by calling fit(). The code then calls transform() on s to fill in the missing values. In this case, the example needs to display the result as a series. To create a series, you must convert the SimpleImputer output to a list and use the resulting list as input to Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing and Dicing : Filtering and Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not need to work with all the data in a dataset. In fact, looking at just one particular column might be benificial, such as age, or a set of rows with a significant amount of information. You perform two steps to obtain just the data you need to perform a particular task :\n",
    "\n",
    "1. Filter rows to create a subject of the data that meets the criterion you select\n",
    "   (such as the people between the ages of 5 and 10)\n",
    "\n",
    "2. Select data columns that contain the date you need to analyze. For example you probably don't need the individual's names unless you want to perform some analysisbased of name.\n",
    "\n",
    " The act of slicing and dicing data, gives you a subset of the data suitable for analysis. The following sections describe various ways to obtain specific pieces of data to meet particular needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing can occur in multiple when working with data, but the technique of interest in this section is to slice from row of 2-D or 3-D data. A 2-D array may contain temperatures ( x axis) over a specific time frame (y axis). Slicing a row would mean seeing temperaturres at a specific time. In some cases, you might associate rows with cases in a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3-D array might include an axis for place (x axis), product (Y axis), and time (z axis) so that you can see sales for items over time. Perhaps you want to track whether the sales of an item are increasing, and specifically where they are increasing. Slicing a row would mean would mean seeing all the sales for one specific product for all locations at any time. The following example demonstrates how to perform this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 12, 13],\n",
       "       [14, 15, 16],\n",
       "       [17, 18, 19]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array ([[[1, 2, 3], [4, 5, 6], [7, 8, 9],],\n",
    "              [[11,12,13], [14,15,16], [17,18,19],],\n",
    "              [[21,22,23], [24,25,26], [27,28,29]]])\n",
    "\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the example builds a 3-D array. It then slice row 1 of that array to produce the above output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the examples from previous sections, slicing columns would obtain data at a 90- degree angle from rows. In other words, when working with the 2-D array, you would want to see the times at which specific temperaturs occured.\n",
    "Likewise, you might want to see the sales of all product for a specific location at any time when working with the 3-D array. In some cases, you might associate columns with features in a dataset. The following exaple demonstrates how to perform this task using the same array as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6],\n",
       "       [14, 15, 16],\n",
       "       [24, 25, 26]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array ([[[1, 2, 3], [4, 5, 6], [7, 8, 9],],\n",
    "              [[11,12,13], [14,15,16], [17,18,19],],\n",
    "              [[21,22,23], [24,25,26], [27,28,29]]])\n",
    "\n",
    "x[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indexing now occurs at two levels. The first index refers to the row. Using the colon (:) for the row means to use all the rows. The second index refers to a column. In this case, the output will contain column 1. As seen in the ouput above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember : This is a 3-D array. Therefore, each of the columns contains all the z axis elements. What you see is every row -  0 through 2 for that column with every z axis element 0 through 2 for that column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The act of dicing means to perform both row and column slicing such that you end up with a data wedge. For example, when working with the 3-D array, you might want to see the sales of a specific product in a specific location at any time. The following example demostrates how to perform this task using the same array as in the previous two sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 15 16]\n",
      "[ 5 15 25]\n",
      "[12 15 18]\n",
      "\n",
      "[[[14 15 16]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array ([[[1, 2, 3], [4, 5, 6], [7, 8, 9],],\n",
    "              [[11,12,13], [14,15,16], [17,18,19],],\n",
    "              [[21,22,23], [24,25,26], [27,28,29]]])\n",
    "\n",
    "print(x[1,1])\n",
    "print(x[:,1,1])\n",
    "print(x[1, :, 1])\n",
    "print()\n",
    "print(x[1:2, 1:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example dices the array in four different ways. First you get row 1, column 1.\n",
    "Of course, what you may actually want is column 1, z axis 1. If thats not quite right, you could always request row 1, z axis 1 instead. then again, you maywant row 1 and 2 of column 1 and 2. Check the output above for all request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating and Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used for data science purposes seldom comes in a neat package. You may need to work with multiple databases in various locations -- each of which has its own data format. it's impossible to perform analysis on such disparate sources of information with any accuracy. To make the data useful, you must create a single dataset (by concatenating, or combining, the data from various source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Part of the process is to ensure that each field you create for the combined dataset has the same characteristics. For example, an age field in one database might appear as a string, but another database could use an integer for the same field. For the field to work together, they must appear as the same type of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section help you understand the process involve in concatenating and transforming data from various sources to create a single dataset. After you have a single dataset from these sources, you can begin to perform task such as analysis on the data. Of course, the trick is to create a single dataset that truly represents the data in all those disparate datasets -- modifying the data would result in skewed result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding new cases and variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You often find a need to combine dataset in various ways or even to add new information for the sake of analysis purposes. The result is a combined dataset that includes either new cases or variables. The following example show techniques for performing both tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  3  2  3\n",
      "2  1  3  4\n",
      "3  4  4  4\n",
      "\n",
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  3  2  3\n",
      "2  1  3  4\n",
      "3  4  4  4\n",
      "4  5  5  5\n",
      "\n",
      "   A  B  C  D\n",
      "0  2  1  5  1\n",
      "1  3  2  3  2\n",
      "2  1  3  4  3\n",
      "3  4  4  4  4\n",
      "4  5  5  5  5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [2,3,1],\n",
    "                   'B': [1,2,3],\n",
    "                   'C': [5,3,4]})\n",
    "\n",
    "df1 = pd.DataFrame({'A': [4],\n",
    "                    'B': [4],\n",
    "                    'C': [4]})\n",
    "\n",
    "\n",
    "df = df.append(df1)\n",
    "df = df.reset_index(drop=True)\n",
    "print(df)\n",
    "\n",
    "\n",
    "df.loc[df.last_valid_index() + 1] = [5, 5, 5]\n",
    "print()\n",
    "print(df)\n",
    "\n",
    "df2 = pd.DataFrame({'D': [1, 2, 3, 4, 5]})\n",
    "\n",
    "df = pd.DataFrame.join(df, df2)\n",
    "print()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to add more data to an existing DataFrame is to rely on the append() method. you can also use the concat() method. In this case, the three cases found in df are added to single case found in df1. To ensure that the data is appended as anticipated, the columns in df and df1 must match. When you append two DataFrame objects in this manner, the new DataFrame contains the old index values. Use the reset_index() method to create a new index to make accessing cases easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add another case to an existing DataFrame by creating the new case directly. Any time you add a new entry at a position that is one greater than the last_valid_index(), you get a new case result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to add a new variable (column) to the DataFrame. In this case, you rely on join() to perform the task. The resulting DataFrame will match cases with the same index value, so indexing is important. In addition, unless you want blank value, the number of cases in both DataFrame objects must match. Take a look at the output above for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point, you may need to remove cases or variables from a dataset because they aren't requires for your analysis. In both cases, you rely on the drop() method to perform the task. The difference in removing cases or variables is in how you describe what to remove, as shown in the following example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  3  2  3\n",
      "2  1  3  4\n",
      "\n",
      "   A  B  C\n",
      "0  2  1  5\n",
      "2  1  3  4\n",
      "\n",
      "   A  C\n",
      "0  2  5\n",
      "2  1  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [2,3,1],\n",
    "                   'B': [1,2,3],\n",
    "                   'C': [5,3,4]})\n",
    "print(df)\n",
    "\n",
    "df = df.drop(df.index[[1]])\n",
    "print()\n",
    "print(df)\n",
    "\n",
    "\n",
    "df = df.drop('B', 1)\n",
    "print()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example begins by removing a case from df. Notice how the code relies on an index to describe what to remove. You can  remove just one case(as shown), ranges of cases, or individual cases separated by commas. The main concern is to ensure that you have the correct index numbers for the cases you want to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing a column is different. This example shows how to remove a column using a column name. You can also remove a column by using an index. In both cases, you must specify an axis as part of the removal process ( normally 1 ). Take a look at the output above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting and shuffling are two ends of the same goal -- to manage data order. In the first case, you put the data in order, while in the second, you remove any systematic patterning from the order. In general, you don't sort datasets for the purpose of analysis because doing so can cause you to get incorrect results. However, you might want to sort data for presentation purposes. The following example code shows both Sorting and Shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  1  2  3\n",
      "2  2  3  4\n",
      "3  3  5  1\n",
      "4  3  4  1\n",
      "5  5  2  2\n",
      "6  4  5  3\n",
      "\n",
      "   A  B  C\n",
      "0  1  2  3\n",
      "1  2  1  5\n",
      "2  2  3  4\n",
      "3  3  4  1\n",
      "4  3  5  1\n",
      "5  4  5  3\n",
      "6  5  2  2\n",
      "\n",
      "   A  B  C\n",
      "0  2  1  5\n",
      "1  4  5  3\n",
      "2  3  5  1\n",
      "3  5  2  2\n",
      "4  1  2  3\n",
      "5  2  3  4\n",
      "6  3  4  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [2,1,2,3,3,5,4],\n",
    "                   'B': [1,2,3,5,4,2,5],\n",
    "                   'C': [5,3,4,1,1,2,3]})\n",
    "print(df)\n",
    "\n",
    "df = df.sort_values(by=['A', 'B'], ascending=[True, True])\n",
    "df = df.reset_index(drop=True)\n",
    "print()\n",
    "print(df)\n",
    "\n",
    "\n",
    "index = df.index.tolist()\n",
    "np.random.shuffle(index)\n",
    "df = df.loc[df.index[index]]\n",
    "df = df.reset_index(drop=True)\n",
    "print()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it turns out that sorting the data is a bit easier than shuffling it. To sort the data, you use the sort_value() method and define which columns to use for indexing purposes. You can also determine whether the index is ascending or descending order. Make sure to always call reset_index() when you're done so that the index appears in order for analysis or other purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shuffle the data, you first acquire the current index using df.index.tolist() and place index. A call to random_shuffle() creates a new order for the index. You then apply the new order to df using loc[]. As always, you call reset_index() to finalize the new order. Take a look at the output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Data at Any Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation is the process of combining or gruping data together into a set, bag, or list. The data may not be alike. However, in most cases, an aggregation function combines several rows together statiscally using algorthms such as average, count, maximum, median, minimum, mode, or sum. There are several reason to aggregate data:\n",
    "\n",
    "\n",
    "1. Make it easier to analyze\n",
    "2. Reduce the ability of anyone to deduce the data of an individual from the dataset for privacy or other reasons\n",
    "3. Create a combined data element from one data element from one data source that matches a combined data element in another source\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important use of data aggregation is to promote anonymity in order to meet legal or other concerns. Sometimes even data that should be anonymous turns out to provide identification of an individual using the proper analysis techniques. For example, researchers have found that it's impossible to identify individuals based on just three credit card purchases ( see https://www.computerworld.com/article/2877935/how-three-small-credit-card-transactions-could-reveal-your-identity.html for details). Here's an example that shows how to perform aggreagation task :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Map  Values  S    M    V\n",
      "0    0       1  6  2.0  1.0\n",
      "1    0       2  6  2.0  1.0\n",
      "2    0       3  6  2.0  1.0\n",
      "3    1       5  9  4.5  0.5\n",
      "4    1       4  9  4.5  0.5\n",
      "5    2       2  7  3.5  4.5\n",
      "6    2       5  7  3.5  4.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Map': [0,0,0,1,1,2,2],\n",
    "                   'Values': [1,2,3,5,4,2,5]})\n",
    "\n",
    "df['S'] = df.groupby ('Map')['Values'].transform(np.sum)\n",
    "df['M'] = df.groupby ('Map')['Values'].transform(np.mean)\n",
    "df['V'] = df.groupby ('Map')['Values'].transform(np.var)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you have two initial features for this DataFrame. The values in Map define which element in Values belong together. For example, when calculating a sum for Map index 0, you use the Values 1, 2, and 3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the aggregation, you must first call groupby() to group the Map values. You then index into Values and rely on transfor() to create the aggregated data using one of several algorithms found in NumPy, such as np.sum. Look at the output above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
